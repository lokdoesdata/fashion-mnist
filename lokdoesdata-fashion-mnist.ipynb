{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ist718': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2b6ac2a62dddf6a7efd67d558055a2a254486810fb954c9dc6dee05d5512019c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lokdoesdata/fashion-mnist/blob/master/lokdoesdata-fashion-mnist.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook requires GPU runtime. Go to **Runtime** > **Change runtime type**  and set **Hardware accelerator** to **GPU** "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduction\n",
    "\n",
    "Image classification is the task of categorizing images into different classes. There are many use cases for image classification. For example:\n",
    "\n",
    "- [Surveillance through facial recognition](https://www.economist.com/united-states/2021/03/09/america-grapples-with-regulating-surveillance-technology)\n",
    "- [Detecting cancer cells](https://www.news-medical.net/news/20210310/Researchers-develop-AI-based-tissue-section-analysis-system-to-diagnose-breast-cancer.aspx)\n",
    "- [Automated detection of corrosion](https://www.sciencedirect.com/science/article/pii/S1738573320302266)\n",
    "\n",
    "In practice, the biggest challenge in image classification is the collection of labeled data. However, in this exercise, the popular Fashion MNIST dataset was used to showcase different techniques that can be used for image classification.\n",
    "\n",
    "This was originally done as part of a course assignment for Big Data Analytics (IST 718) at Syracuse University."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fashion MNIST Dataset\n",
    "\n",
    "The Fashion MNIST dataset is an alternative to the popular digit MNIST dataset.  This dataset contains 70,000 28x28 grayscale images in 10 fashion categories.  60,000 of which are in the train set, and 10,000 of which are in the test set.  The dataset can be obtained [here](https://github.com/zalandoresearch/fashion-mnist). It can also be retrieved through the [TensorFlow's API](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data).\n",
    "\n",
    "The 10 fashion categories in the Fashion MNIST dataset:\n",
    "\n",
    "| Label | Description |\n",
    "| :---: | :---------- |\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9     | Ankel boot  |\n",
    "\n",
    "The Fashion MNIST dataset is a perfectly balanced dataset with even number of observations per class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Set Up\n",
    "\n",
    "This notebook uses [`cuDF`](https://docs.rapids.ai/api/cudf/stable/) and [`cuML`](https://docs.rapids.ai/api/cuml/stable/) by [RAPIDS](https://rapids.ai/), and it's designed to run on Google Colab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Install RAPIDS 0.18\n",
    "\n",
    "This clones the Github Repository for RAPIDS and uses a bash script to install RAPIDS on Google Colab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!bash rapidsai-csp-utils/colab/rapids-colab.sh 0.18\n",
    "import sys, os\n",
    "\n",
    "dist_package_index = sys.path.index('/usr/local/lib/python3.7/dist-packages')\n",
    "sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.7/site-packages'] + sys.path[dist_package_index:]\n",
    "sys.path\n",
    "exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())"
   ]
  },
  {
   "source": [
    "### Install LightGBM\n",
    "\n",
    "This install the GPU version of [`LightGBM`](https://lightgbm.readthedocs.io/en/latest/) on Google Colab. `LightGBM` is a Gradient Boosting Tree-based Model developed by Microsoft."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lightgbm --install-option=--gpu"
   ]
  },
  {
   "source": [
    "### Import Packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# data manipulation\n",
    "import cudf\n",
    "import numpy as np # could also try using cupy\n",
    "\n",
    "# additional models\n",
    "from cuml.manifold import TSNE # t-distributed stochastic neighbor embedding\n",
    "import cuml\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (roc_curve, auc, roc_auc_score, confusion_matrix,\n",
    "                             precision_score, recall_score, accuracy_score)\n",
    "\n",
    "# visuals\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "source": [
    "## Data\n",
    "\n",
    "The 10 labels in the Fashion MNIST dataset ordered by their index number."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankel boot'\n",
    "]"
   ]
  },
  {
   "source": [
    "### Load Data from TensorFlow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "y_test_binarize = label_binarize(y_test, classes=list(range(10)))"
   ]
  },
  {
   "source": [
    "## Exploratory Data Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Visualize the Dataset\n",
    "\n",
    "This script below was taken from Kaggle and it was used to visualize the digit MNIST dataset. It works just fine for the Fashion MNIST dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MNIST(instances, images_per_row=10):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary)\n",
    "    plt.axis(\"off\")\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X_train[:25]\n",
    "plot_MNIST(example_images, images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Figure above shows 25 sample images from the fashion MNIST dataset. There are a variety of different appeals in the dataset for each category.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### T-Distributed Stochastic Neighbor Embedding (tSNE)\n",
    "\n",
    "tSNE is a popular visualization tool to visualize high dimensional dataset. It is perfect for Fashion MNIST as it has 784 features.  This is done using the cuML library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(b):\n",
    "\n",
    "  # output.clear_output()\n",
    "\n",
    "  _n_iter = n_iter.value\n",
    "  _perplexity = perplexity.value \n",
    "  _learning_rate = learning_rate.value\n",
    "\n",
    "  tsne = TSNE(\n",
    "      n_components=2,\n",
    "      n_neighbors=4*_perplexity,\n",
    "      perplexity=_perplexity,\n",
    "      learning_rate=_learning_rate,\n",
    "      n_iter=_n_iter,\n",
    "      random_state=718)\n",
    "\n",
    "  data_tsne = tsne.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "  f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "  with sns.axes_style(\"white\"):\n",
    "      sns.despine(f, left=True, bottom=True)\n",
    "\n",
    "      g = sns.scatterplot(\n",
    "          x = data_tsne[:,0],\n",
    "          y = data_tsne[:,1],\n",
    "          hue = [label_index[int(val)] for val in y_train],\n",
    "          ax = ax\n",
    "      )\n",
    "\n",
    "      ax.text(x=0.5, y=1.1, s='Fashion MNIST tSNE', fontsize=20, weight='bold', ha='center', va='bottom', transform=ax.transAxes)\n",
    "      ax.text(x=0.5, y=1.05, s=f'Iterations: {_n_iter} | Perplexity: {_perplexity} | Learning Rate: {_learning_rate}', fontsize=12, alpha=0.75, ha='center', va='bottom', transform=ax.transAxes)\n",
    "      \n",
    "      g.set(xticklabels=[])\n",
    "      g.set(yticklabels=[])\n",
    "      g.tick_params(left=False, bottom=False)\n",
    "      \n",
    "      f.tight_layout()\n",
    "  with output:\n",
    "      output.clear_output()\n",
    "      g"
   ]
  },
  {
   "source": [
    "`ipywidgets` was used to showcase how one could use it as a tool to determine how different parameters affect the tSNE model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}\n",
    "layout = {'width':'500px'}\n",
    "\n",
    "n_iter = widgets.IntSlider(\n",
    "    value=3000, min=500, max=3000, step=100, \n",
    "    description='Number of Iterations:', style=style, layout=layout)\n",
    "perplexity = widgets.IntSlider(\n",
    "    value=50, min=10, max=100, step=10, \n",
    "    description='Perplexity:', style=style, layout=layout)\n",
    "learning_rate = widgets.IntSlider(\n",
    "    value=200, min=100, max=300, step=20, \n",
    "    description='Learning Rate:', style=style, layout=layout)\n",
    "\n",
    "run_tsne = widgets.Button(description='Run tSNE')\n",
    "output = widgets.Output()\n",
    "run_tsne.on_click(tsne_plot)\n",
    "\n",
    "control = widgets.VBox([n_iter, \n",
    "                        perplexity, \n",
    "                        learning_rate, \n",
    "                        run_tsne])\n",
    "display(control, output)"
   ]
  },
  {
   "source": [
    "From the tSNE figure above, it appears that trousers and bags can be easily differentiated from the other apparels. The three footwears, ankle boots, sneakers and sandals are separated from the rest of the apparels into its own cluster while maintaining some separation between each other. Pullover, shirt and coat are all clustered together which suggest that it could be difficult to classify them. Shirt, in particular, look somewhat dispersed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Models\n",
    "\n",
    "Two classification models were created to classify the Fashion MNIST images. They are LightGBM by Microsoft and ConvNet by Keras (TensorFlow). \n",
    "\n",
    "Both models were trained with a conventional train-test split."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### LightGBM\n",
    "\n",
    "Using a DART booster and a multiclass logistic loss function, LightGBM was able to achieve an 88.9% testing accuracy. This accuracy was achieved with the default parameters with no changes to the number of bins, leaves, iterations or learning rate. As LightGBM supports computation on GPU, this model was trained with an NVIDIA Tesla T4. The computational performance was acceptable, completing in just 1 minutes and 40 seconds.  \n",
    "\n",
    "Additional information on LightGBM is below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict(\n",
    "    boosting='dart',\n",
    "    objective='multiclass',\n",
    "    metrics='multi_logloss',\n",
    "    verbose=1,\n",
    "    random_state=718,\n",
    "    device='gpu',\n",
    "    num_class=len(label_index)\n",
    ")\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_test = lgb.Dataset(X_test.reshape(X_test.shape[0],-1), y_test, reference=lgb_train)\n",
    "\n",
    "lgb_model = lgb.train(lgb_params, lgb_train)"
   ]
  },
  {
   "source": [
    "#### Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "y_prob_lgb_train = lgb_model.predict(X_train.reshape(X_train.shape[0], -1))\n",
    "y_pred_lgb_train = [np.argmax(row) for row in y_prob_lgb_train]\n",
    "\n",
    "# Test\n",
    "y_prob_lgb = lgb_model.predict(X_test.reshape(X_test.shape[0],-1))\n",
    "y_pred_lgb = [np.argmax(row) for row in y_prob_lgb]"
   ]
  },
  {
   "source": [
    "#### Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### Training versus Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"LightGBM Performance on Train Set\n",
    "Accuracy: {round(accuracy_score(y_train, y_pred_lgb_train),3)}\n",
    "Precision (weighted): {round(precision_score(y_train, y_pred_lgb_train, average='weighted'),3)}\n",
    "Recall (weighted): {round(recall_score(y_train, y_pred_lgb_train, average='weighted'),3)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"LightGBM Performance on Test Set\n",
    "Accuracy: {round(accuracy_score(y_test, y_pred_lgb),3)}\n",
    "Precision (weighted): {round(precision_score(y_test, y_pred_lgb, average='weighted'),3)}\n",
    "Recall (weighted): {round(recall_score(y_test, y_pred_lgb, average='weighted'),3)}\"\"\")"
   ]
  },
  {
   "source": [
    "| LightGBM Performance | Train       | Train       |\n",
    "| :------------------- | :---------: | :---------: |\n",
    "| Accuracy             | 95.8%       | 88.9%       |\n",
    "| Precision (Weighted) | 95.8%       | 88.9%       |\n",
    "| Recall (Weighted)    | 95.8%       | 88.9%       |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### Confusion Matrix\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_lgb)\n",
    "sns.heatmap(conf_mat, xticklabels=label_index, yticklabels=label_index,\n",
    "            annot=True, annot_kws={\"size\": 16}, fmt='d', cbar=False, ax=ax)\n",
    "\n",
    "ax.text(x=0.5, y=1.05, s='LightGBM - Confusion Matrix', fontsize=20, weight='bold', ha='center', va='bottom', transform=ax.transAxes)"
   ]
  },
  {
   "source": [
    "The confusion matrix shows that the model was capable to predict most of the fashion categories. It struggles with “shirt”, as it was suggested by the tSNE analysis. Shirt and T-shirt, in particular, has the highest misclassification rate between each other. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### ROC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i, label in enumerate(label_index):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarize[:, i], y_prob_lgb[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarize.ravel(), y_prob_lgb.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(10):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= 10\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "for i, label in enumerate(label_index):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,\n",
    "             label=f'{label} (area = {round(roc_auc[i],2)})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('LightGBM - ROC curves of individual classes in Fashion MNIST')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "The LightGBM model’s ROC curve for each of the ten categories in the Fashion MNIST dataset.  With a 0.96 AUC, shirt is the worse of the ten categories.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Keras ConvNet\n",
    "\n",
    "A 7 layers sequential ConvNet was created: \n",
    "\n",
    "1.\tConvolutional Layer with 32 filters and a (3,3) filter size.  This layer uses the rectified linear activation function.\n",
    "2.\tMax pooling layer\n",
    "3.\tConvolutional Layer with 64 filters and a (3,3) filter size.  This layer uses the rectified linear activation function.\n",
    "4.\tMax pooling layer\n",
    "5.\tFlatten\n",
    "6.\tDropout with 50% probability\n",
    "7.\tDense layer with 10 nodes.  This layer uses the softmax activation function.  \n",
    "\n",
    "The ConvNet uses a stochastic gradient descent optimizer with a 0.01 learning rate and a momentum of 0.9 and trained with a multiclass logistic loss function (categorical cross-entropy). 10% of the training data was held back for validation to avoid data leakage. With a batch size of 128, and an epoch of 15, the model was trained in 53.8s.\n",
    "\n",
    "Keras's ConvNet was able to achieve an 88.8% testing accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_keras = np.expand_dims(X_train, -1)\n",
    "X_test_keras = np.expand_dims(X_test, -1)\n",
    "\n",
    "y_train_keras = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_keras = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "keras_model.compile(loss=\"categorical_crossentropy\", \n",
    "                    optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9), \n",
    "                    metrics=[\"accuracy\"])\n",
    "keras_model.fit(X_train_keras, y_train_keras, batch_size=128, epochs=15, validation_split=0.1)"
   ]
  },
  {
   "source": [
    "#### Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "y_prob_keras_train = keras_model.predict(X_train_keras)\n",
    "y_pred_keras_train = [np.argmax(row) for row in y_prob_keras_train]\n",
    "\n",
    "# Test\n",
    "y_prob_keras = keras_model.predict(X_test_keras)\n",
    "y_pred_keras = [np.argmax(row) for row in y_prob_keras]"
   ]
  },
  {
   "source": [
    "#### Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### Training versus Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Keras ConvNet Performance on Train Set\n",
    "Accuracy: {round(accuracy_score(y_train, y_pred_keras_train),3)}\n",
    "Precision (weighted): {round(precision_score(y_train, y_pred_keras_train, average='weighted'),3)}\n",
    "Recall (weighted): {round(recall_score(y_train, y_pred_keras_train, average='weighted'),3)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Keras ConvNet Performance on Test Set\n",
    "Accuracy: {round(accuracy_score(y_test, y_pred_keras),3)}\n",
    "Precision (weighted): {round(precision_score(y_test, y_pred_keras, average='weighted'),3)}\n",
    "Recall (weighted): {round(recall_score(y_test, y_pred_keras, average='weighted'),3)}\"\"\")"
   ]
  },
  {
   "source": [
    "| Keras ConvNet Performance | Train       | Train       |\n",
    "| :------------------------ | :---------: | :---------: |\n",
    "| Accuracy                  | 89.9%       | 88.8%       |\n",
    "| Precision (Weighted)      | 89.9%       | 88.7%       |\n",
    "| Recall (Weighted)         | 89.9%       | 88.8%       |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### Confusion Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_keras)\n",
    "sns.heatmap(conf_mat, xticklabels=label_index, yticklabels=label_index,\n",
    "            annot=True, annot_kws={\"size\": 16}, fmt='d', cbar=False, ax=ax)\n",
    "\n",
    "ax.text(x=0.5, y=1.05, s='Keras ConvNet - Confusion Matrix', fontsize=20, weight='bold', ha='center', va='bottom', transform=ax.transAxes)"
   ]
  },
  {
   "source": [
    "The ConvNet also struggled with “shirt” and did worse than LightGBM. It, however, did slightly better across the other categories. With comparable performance but significantly better fitting time, ConvNet is argumentively the better model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### ROC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i, label in enumerate(label_index):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarize[:, i], y_prob_keras[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarize.ravel(), y_prob_keras.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(10):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= 10\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "for i, label in enumerate(label_index):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,\n",
    "             label=f'{label} (area = {round(roc_auc[i],2)})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Keras ConvNet - ROC curves of individual classes in Fashion MNIST')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "The ROC curve is also strikingly similar to the LightGBM’s model. With a 0.96 AUC, shirt is also the worse of the ten categories for ConvNet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusion\n",
    "\n",
    "Both LightGBM and Keras' ConvNet were able to achieve close to 90% accuracy with very basic techniques. Hyperparameter tuning would most certainly improve their accuracy.\n",
    "\n",
    "tSNE is a very useful visualization tool for high dimensional dataset. It illustrated how it would be difficult to differentiate shirt from the other categories."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}